\section{Trajectory Learning}\label{sec: traj learning}

\subsection*{2014 - Sequence to Sequence Learning with Neural Networks}

\textbf{Name: seq2seq}

\cite{sutskever2014sequence}

\subsection*{2017 - Identifying Human Mobility via Trajectory Embeddings}

\cite{gao2017identifying} classifies users based on trajectory data. The problem is hard because there are many more trajectories than users.

\gls{rnn} is used, and said to be good for classification when the number of labels is small. In particular uses a \gls{lstm} for processing sub-trajectories.

There is a location embedding, not sure how that is computed. But the trajectories are points on google maps, so there maybe be semantic information in there.
%
The sequence of location embedding is passed onto the \gls{lstm}, not sure how they handle different trajectory lengths.


\subsection*{2018 - Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings}

\textbf{Code on:} \url{https://github.com/wyndwarrior/Sectar}

\cite{co2018self} learns an embedding for trajectories with one encoder and two decoders: a state decoder to decode from the latent space back into trajectories, and a policy decoder, which generates the trajectory in the environment. As such the state decoder predict the trajectory of the policy. The encoder is used in a hierarchical \gls{rl} setup.

The state encoder and decoder are \glspl{rnn} and the policy decoder is a feed-forward \gls{nn}.

\textbf{note:} if the encoder is trained with trajectories from different tasks, the policy will be conditioned to each task, what is sort of parameterzing the policy to tasks. 

In the paper, the policy has unknown dynamics, and hence the \gls{rl} setup. Trajectories are continuos poses of joints over time. Tested in simulation. 

\subsection*{2018 - Anomalous Trajectory Detection Using Recurrent Neural Network}

\textbf{Code on:} \url{https://github.com/LeeSongt/ATD-RNN}

\cite{song2018anomalous} proposes anomalous trajectory detection using \gls{rnn}.

The trajectories are discretized, using a grid, and feed to a stacked \gls{rnn} for learning the embedding, then a multi-layered perceptron and a soft-max layer detects if the trajectory is anomalous.
%
The stacked \gls{rnn} is made by feeding the hidden states od the previous to the next \gls{rnn}.

The trajectories are padded in order to get trajectories of the same length.

\gls{lstm} and \gls{gru} are two special types of \glspl{rnn} are tested. \glspl{gru} seems to work better.

\subsection*{2018 - Deep Representation Learning for Trajectory Similarity Computation}

\textbf{Name: t2vec}
\textbf{Code on:} \url{https://github.com/boathit/t2vec}

\cite{li2018deep} presents \texttt{t2vec}. A \gls{dl} approach for trajectory similarity. States that using \gls{rnn} is not a very good idea because you cannot reconstruct the trajectory and it fails to consider spatial proximity, which is inherited in trajectory data.

called in the paper as \texttt{t2vec} or \texttt{seq2seq}?

The approach is based on the encoder-decoder framework.
%
Handling varying sampling rates is done by augmenting the training data creating sub-trajectories by sub-sampling and noise addition. They also propose a spatial-aware loss, and pre-train the ??cells?? and let them to be optimized during training.

\textbf{Notes:} The paper is very confusing. I do not really know that are the inputs and outputs or how the sequences are fed in the \gls{rnn} inside the encoder.


\subsection*{2020 - Trembr: Exploring Road Networks for Trajectory Representation Learning}

\cite{fu2020trembr} presents \texttt{traj2vec}. The paper focus on trajectories on roads. It preprocesses the trajectories by projecting them in a road network and the trajectory is a sequence of road segments and travel time.

The \gls{rnn} decoder is conditioned to the road network, and the training is made by optimising a loss for the trajectory and another for time.

\textbf{Notes:} Maybe the secret for velocities profiles is in the addition of time to the loss.

\subsection*{2019 - Computing Trajectory Similarity in Linear Time: A Generic Seed-Guided Neural Metric Learning Approach}

\textbf{Name: NeuTraj}

\textbf{Code on:} \url{https://github.com/yaodi833/NeuTraj}

\cite{yao2019computing} proposes a method for accelerating trajectory similarity computation by sampling seeds of trajectories, computing their similarity, and approximating them with a neural metric.

States that \glspl{rnn}, \glspl{lstm}, and \glspl{gru} can only  model one sequence without considering the between-sequence correlation.

Does not consider time in the trajectory. Starts sampling from the trajectories and computes a distance matrix between the samples using a given trajectory distance metric which is then normalized.

The \gls{rnn} is augmented with a memory, which is created by dividing the space into a grid, and for each grid slot, the memory stores the hidden vector of the \gls{rnn}. This memory is used to extend the \gls{rnn} cell, sort of like an \gls{lstm}.

The loss for training is $\mathcal{L}{\tau_i, \tau_j}=\sum_k w_k(f(\tau_i, \tau_j)-\exp(-||e_i-e_j||)$, a weighted difference between the similarity metric $f$ and the distance in the embedding space ($e_i-e_j$). The weight $w_k$ is obtained using the normalized distance matrix, computing pairs of similar and dissimilar trajectories and more fancy stuff.

\textbf{Notes:} map is like google map.

\subsection*{2020 - Trajectory similarity learning with auxiliary supervision and optimal matching}

\textbf{Name:Traj2SimVec}

\cite{zhang2020trajectory} follows the same idea as in \cite{yao2019computing} which selects some trajectories for pre-training \rev{something}, the training samples are divided in three sub-trajectories \rev{because it seems to help learning}.

A distance matrix is computed which is used as supervision signal, similar to \cite{yao2019computing}.

\subsection*{2020 - MARC: a robust method for multiple-aspect trajectory classification via space, time, and semantic embeddings}

\cite{may2020marc} Embeds semantics on the trajectories. Each semantic information (weather, time, type of place) has an encoding, and a weight matrix which transform them into a fixed size vector. The semantic trajectory is fed to an \gls{lstm}, which encodes the trajectories, having the hidden states used for classification.

\subsection*{2021 - Embedding-Based Similarity Computation for Massive Vehicle Trajectory Data}

\cite{chen2021embedding} seems to propose the exact same thing as \cite{yao2019computing}, but with interpolation for de-noising.

\subsection*{2021 - STENet: A hybrid spatio-temporal embedding network for human trajectory forecasting}

\cite{zhang2021stenet} Focuses on predicting pedestrian trajectories. Uses a \gls{lstm} with \glspl{cnn} to embed position features in multiple temporal time-scales. The encoder-decoder structure stack a \gls{cnn} and a graph attention model. The decodes stacks many \glspl{lstm}. 

They give related works on social trajectory learning.

\textbf{Notes:} They point to \glspl{vae} for modelling multi-modality and for the generative capabilities. 

\subsection*{2021 - A Graph-Based Approach for Trajectory Similarity Computation in Spatial Networks}

\cite{han2021graph} Propose a \gls{gnn}-based trajectory embedding. The framework measures trajectory similarities, learns \glspl{poi}, and leans a trajectory embedding.

A trajectory is encoded as the points in a graph map. Then they define a trajectory similarity metric on the \gls{poi} graph, based on the graph distance between the points and trajectories. An embedding capturing the neighbours and graph trajectory is learned. The \gls{poi} embeddings and their neighbours are used to learn another embedding using its neighbours information. Finally, \glspl{lstm} are used to learn the trajectory over the graph embeddings. The loss function minimizes the above defined distance between trajectories and the distance between the two closest trajectories.

\subsection*{2021 - T3S: Effective Representation Learning for Trajectory Similarity Computation}

\textbf{Name: T3S}

\cite{yang2021t3s} combines \glspl{lstm} and attention \glspl{nn} over the grid graph for learning the embedding. Close to \cite{yao2019computing, zhang2020trajectory, chen2021embedding}.

\subsection*{2021 - How meaningful are similarities in deep trajectory representations?}

\textbf{Code on:} \url{https://dbis.ipd.kit.edu/2652.php}

\cite{taghizadeh2021meaningful} presents a survey and evaluation of \texttt{t2vec} \cite{li2018deep} and other methods. Seems like \texttt{t2vec} with some variations outperform the rest. \texttt{t2vec} seems to be stacked \glspl{lstm}.

Evaluate how changing \texttt{t2vec} parameters affect similarity values. \texttt{t2vec} seems robust to parameters.

Evaluate \texttt{t2vec} against non learning metrics. Seems like associating them lead to better results.

\rev{They DO ignore the whole literature on learning methods?}

Concludes that using \gls{lcss} and \texttt{t2vec} leads to a better trajectory similarity, covering overlap, shape, direction and distance.

\textbf{Notes:} Maybe that should be $4$ characteristics to consider.   


\subsection*{2022 - Spatio-Temporal Trajectory Similarity Learning in Road Networks}

\textbf{Name: ST2vec}

\cite{fang2022spatio} learns a spatio-temporal representation. Two steps, which is based on learning a spatial model, a temporal model and a co-attention fusion module. It is based on a road network, trajectories are sequences of vertex on the road network.

Define the distance of a spatio-temporal trajectory as a weighted sum for a spatio-distance ($d_s$) and a temporal distance ($d_t$):

$$d(\tau_i, \tau_j) = \alpha d_s(\tau_i, \tau_j) + (1-\alpha)d_t(\tau_i, \tau_j) | \alpha \in [0,1]$$

Later uses \glspl{lstm} to learn using two strategies, using one \gls{lstm} for space and another for time, or using one for both.

\subsection*{2022 - Deep Fuzzy Contrast-Set Deviation Point Representation and Trajectory Detection}
\cite{ahmed2022deep} Grid-map based, contrastive learning. 

\textbf{notes:} hard to understand what they are doing here.

\subsection*{2022 - Contrastive Pre-training of Spatial-Temporal Trajectory Embeddings}
\cite{lin2022contrastive} employs contrastive learning for learning an embedding which retains high-level travel semantics.

Recovering the original trajectory is not a good approach when learning representations with \glspl{rnn} since it fails to capture the high-level information of trajectories. Contrastive learning with noisy augmentation can handle the high-level information while being robust to noise. However data augmentation needs to be well designed. 

The positive samples are created with subsampling the query trajectory, while the negative samples come from different trajectories.

\textbf{Notes:} Not sure this is correct, I think the ``different trajectories'' should be far enough from the query trajectory to be a negative sample.

The encoder stacks a spatio-temporal encoding layer and attention layers. For the first, a learnable encoding of locations is learned (each location leads to a vector) and location and time are passed to a trigonometric vector transformation to compute features which can capture periodic information; those vectors are then summed up. The attention layer is actually 2 stacked attention layers.


\subsection*{2022 - TMN: Trajectory Matching Networks for Predicting Similarity}
\cite{yang2022tmn} uses attention to compute intra-trajectory similarities, and then uses a \gls{lstm}.

\textbf{Notes:} Comparison ignores many methods.

\subsection*{2022 - TSNE: Trajectory Similarity Network Embedding}
\cite{ding2022tsne} uses a pre-defined trajectory measure function to construct a k-NNG (K nearest neighbours graph) and computes the embedding based on the graph.

\textbf{Notes:} Not sure how they compute the embedding from the graph. Seems like the graph representation allows to handle partial similarity and unordered similarity.

\subsection*{2022 - Towards robust trajectory similarity computation: Representation-based spatio-temporal similarity quantification}
\textbf{Name: RSTS} 

\cite{chen2022towards} splits the spatio-temporal trajectories into cells, and uses a triplet loss for the learning.
%
It enforces that if the time and space similarities are higher, then the distance in the encoded space must be smaller, and that, in the encoded space, the distance between two trajectories variations (noise and downsampling) must obey the distance of the trajectories.

An embedding is used for the tokens, which are then passed to a \gls{rnn} encoder-decoder. The tokens for the embedded are an ID computed by splitting the space-time into cells. The input is grid-cells (gps + time).

\textbf{Notes:} Analysis is poor. Ignores all other works on learning. Seems like there is little innovation besides the loss.


\subsection*{2023 - Spatial-temporal fusion graph framework for trajectory similarity computation}

\textbf{Name: GTS}

\cite{zhou2023spatial} first learns a point of interest representation on the road network, which is passed to a \gls{gnn} for learning neighbours information as embeddings, and then a \gls{lstm} for learning the sequencing.



A symmetric distance  between trajectories is defined based on the distance between each point of the trajectories and the other trajectory:
$$
d(\tau_1, \tau_2) = \sum_{v\in\tau_1}e^{-d(v, \tau_2)} + \sum_{v\in\tau_2}e^{-d(v, \tau_1)}
$$

The time is considered in an extension called \texttt{ST-LSTM}, which adds a time one-hot encoding into the gating functions of the \gls{lstm}.

\textbf{Notes:} Comparisons goes as far as \texttt{traj2SimVec} \cite{zhang2020trajectory}.

\subsection*{2023 - GRLSTM: Trajectory Similarity Computation with Graph-Based Residual LSTM}

\textbf{Name: GRLSTM}

\cite{zhou2023grlstm} combines \gls{kge}, \gls{gnn} and a multi-layer residual-\gls{lstm}.
%
\gls{kge} is used to learn a point and relation embeddings for constructing a graph, which is passed to the \gls{gnn} for learning the topology in the point-structure graph. Then the \gls{lstm} is used to learn the embeddings trajectories.
%
Uses two losses: a graph-based loss and a trajectory-base loss.

The input is trajectories in a graph road network. The interesting thing here is that adjacent points in the trajectory may not be adjacent in the graph (due to data loss or lower sample rate).

The stacked \gls{lstm} is augmented with a residual layer for handling the gradient forgetting of traditional \gls{lstm}. It is stated that it does not add parameters so it does not affect training time considerably.

\textbf{Notes:} does not really say how the residual function is computed. Similarly to \cite{han2021graph} they implement point and trajectory distances. 

\subsection*{2023 - Contrastive Trajectory Similarity Learning with Dual-Feature Attention}

\textbf{Name: TrajCL}

\cite{chang2023contrastive} introduces four trajectory augmentation and a dual feature self-attention encoder, for learning structural and spatial patterns of trajectories. It does not involve any recurrent structure. Instead, it uses a dual self-attention-based trajectory encoder.

Augmentations:
\begin{description}
	\item[point shifting:] adds an offset to the points
	\item[point masking] randomly removes points from the trajectory
	\item[tuncation] cuts a prefix, suffix, or both from the trajectory
	\item[simplification] uses the Douglasâ€“Peucker algorithm which removes non critical points from the trajectories (like points in a straight line).
\end{description}

The augmented trajectories are used to create two trajectory views to learn structural and spatial features. The augmented trajectories are used to compute two trajectory views.
%
The structural features, the map is converted into a grid, and used to create a graph in which the grid locations are the vertices and the trajectory transitions the edges. Then a graph embedding (\texttt{node2vec}) is used to learn an embedding.
%
For the spatial features, the angle and length of trajectory segments is computed.
%
Both views are augmented by adding a \rev{sketchy} sine and cosine value to the points to capture position information.

Finally the two views are passed to a two-head self attention module to learn the embeddings.




\subsection*{2023 - Spatio-Temporal Trajectory Similarity Measures: A Comprehensive Survey and Quantitative Study}
\cite{hu2023spatio}

\textbf{Code on:} \url{https://github.com/ZJU-DAILY/TSM}

Present a survey with several methods, and benchmark for evaluating them. Apparently \texttt{Traj2SimVec} \cite{zhang2020trajectory} is the learning method, which is not grid-based that handles our problem.