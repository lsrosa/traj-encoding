\section{Trajectory Learning on Robotics}\label{sec: traj learning robotics}

\subsection*{2020 - Controlling Assistive Robots with Learned Latent Actions}

\cite{losey2020controlling} Use encoders to learn latent task representations for assistive robot remote controlling. In this setup, \glspl{vae} are used, encoding states into a task representation, the user gives input from a joystick which are decoded together with the latent space representation.

\textbf{Notes:} Seems like \gls{vae} is used straight up with the trajectories. But it is a bit blurry how the actions are being defined or learned (seem pre defined).

\subsection*{2020 - DiversityGAN: Diversity-Aware Vehicle Motion Prediction via Latent Semantic Sampling}

\cite{huang2020diversitygan} extends \gls{gan} using a low-dimensional approximate semantic (encoding) which is shaped to capture semantics. Sampling from this space allows to cover semantically distinguish outcomes. The work focuses on predicting vehicle trajectories.

An intermediate layer avoids the need of taxonomy \rev{?} by using metric learning, in which a latent representation is trained to match annotations of high-level labels, and forcing the distance to be large if they represent two distinguish semantic labels.
%
The latent space is trained to match human similarity measures.

Past trajectories and map information are embedded, and their embeddings are passed to an \gls{lstm} whose latent space is divided intro a high- and low-level parts. The decoder takes both parts to produce trajectory samples.
%
The trajectory network is a series of fully connected layers that embed a trajectory into a vector \cite{alahi2016social} \rev{seems this work uses \glspl{lstm} for the embeddings}.
%
The map embedding is a fully connected network that maps polynomial coefficients (quadratic) into an embedding.
%
The encoder is a \gls{lstm}, whose hidden states are addeded a Gaussian noise and passed to a non-linear fully-connected network to compute the high and low-level embedding representation. The high-level embedding part is not correlated with the low-level one, and is trained for learning semantic similarities from the human teacher (they use a hand coded oracle though).
%
The decoder is a \gls{lstm}.
%
There is also a discriminator trained for identifying if samples are generated by the architecture or if they are real data.

The loss design incorporates minimal and final displacement losses, a term to enforce the non-correlation between the high and low-level embeddings, and another to enforce that semantically related pairs should also be close in the encoding space.

Sampling is performed using Farthest Point Sampling.

\textbf{Notes:} It is interesting that they added semantics to the network.
 

\subsection*{2021 - Use of Embedding Spaces for Transferring Robot Skills in Diverse Sensor Settings}
\cite{ninomiya2021use}

\subsection*{2022 - STL2vec: Signal Temporal Logic Embeddings for Control Synthesis With Recurrent Neural Networks}
\cite{hashimoto2022stl2vec}

\subsection*{2022 - Learning latent actions to control assistive robots}
\cite{losey2022learning}

\subsection*{2022 - Promoting Quality and Diversity in Population-based Reinforcement Learning via Hierarchical Trajectory Space Exploration}
\cite{miao2022promoting}

\subsection*{2023 - SIRL: Similarity-Based Implicit Representation Learning}
\cite{bobu2023sirl}
